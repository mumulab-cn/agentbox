<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformer架构原理 - 技术卡片</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 40px rgba(0,0,0,0.1);
            overflow: hidden;
        }

        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }

        .header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
            font-weight: 700;
        }

        .header p {
            font-size: 1.2em;
            opacity: 0.9;
        }

        .content {
            padding: 40px;
        }

        .tabs {
            display: flex;
            margin-bottom: 30px;
            border-bottom: 2px solid #f0f0f0;
        }

        .tab {
            padding: 15px 25px;
            cursor: pointer;
            border: none;
            background: none;
            font-size: 16px;
            font-weight: 600;
            color: #666;
            transition: all 0.3s ease;
            border-bottom: 3px solid transparent;
        }

        .tab.active {
            color: #667eea;
            border-bottom-color: #667eea;
        }

        .tab-content {
            display: none;
        }

        .tab-content.active {
            display: block;
        }

        .architecture-diagram {
            text-align: center;
            margin: 30px 0;
            background: #f8f9fa;
            padding: 30px;
            border-radius: 15px;
        }

        .grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .card {
            background: white;
            border-radius: 15px;
            padding: 25px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.08);
            border-left: 5px solid #667eea;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 25px rgba(0,0,0,0.15);
        }

        .card h3 {
            color: #333;
            margin-bottom: 15px;
            font-size: 1.3em;
            display: flex;
            align-items: center;
        }

        .card-icon {
            width: 24px;
            height: 24px;
            margin-right: 10px;
            background: #667eea;
            border-radius: 50%;
            display: inline-flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: bold;
        }

        .card p {
            color: #666;
            line-height: 1.6;
            margin-bottom: 10px;
        }

        .formula {
            background: #f8f9fa;
            padding: 15px;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            margin: 10px 0;
            border-left: 4px solid #667eea;
        }

        .highlight {
            background: linear-gradient(120deg, #a8edea 0%, #fed6e3 100%);
            padding: 2px 6px;
            border-radius: 4px;
            font-weight: 600;
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 5px 15px rgba(0,0,0,0.08);
        }

        .comparison-table th {
            background: #667eea;
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: 600;
        }

        .comparison-table td {
            padding: 15px;
            border-bottom: 1px solid #f0f0f0;
        }

        .comparison-table tr:hover {
            background: #f8f9fa;
        }

        .attention-visual {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            text-align: center;
        }

        .layer-stack {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 20px;
            margin: 20px 0;
        }

        .layer-box {
            background: white;
            border: 2px solid #667eea;
            border-radius: 10px;
            padding: 15px;
            min-width: 120px;
            text-align: center;
            font-weight: 600;
            color: #667eea;
        }

        .arrow {
            font-size: 24px;
            color: #667eea;
        }

        .code-block {
            background: #2d3748;
            color: #e2e8f0;
            padding: 20px;
            border-radius: 10px;
            font-family: 'Courier New', monospace;
            margin: 15px 0;
            overflow-x: auto;
        }

        .detail-section {
            margin: 30px 0;
            padding: 25px;
            background: #f8f9fa;
            border-radius: 15px;
        }

        .detail-section h3 {
            color: #333;
            margin-bottom: 20px;
            font-size: 1.4em;
            border-bottom: 2px solid #667eea;
            padding-bottom: 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>🤖 Transformer架构原理</h1>
            <p>深度学习革命性架构 - 注意力机制的完美实现</p>
        </div>

        <div class="content">
            <div class="tabs">
                <button class="tab active" onclick="showTab('architecture')">架构概览</button>
                <button class="tab" onclick="showTab('attention')">注意力机制</button>
                <button class="tab" onclick="showTab('components')">核心组件</button>
                <button class="tab" onclick="showTab('implementation')">实现细节</button>
            </div>

            <!-- 架构概览 -->
            <div id="architecture" class="tab-content active">
                <div class="architecture-diagram">
                    <svg width="800" height="600" viewBox="0 0 800 600">
                        <!-- 编码器 -->
                        <rect x="50" y="100" width="300" height="400" fill="#e3f2fd" stroke="#1976d2" stroke-width="2" rx="10"/>
                        <text x="200" y="90" text-anchor="middle" font-size="18" font-weight="bold" fill="#1976d2">编码器 (Encoder)</text>
                        
                        <!-- 编码器层 -->
                        <rect x="70" y="120" width="260" height="60" fill="#bbdefb" stroke="#1976d2" rx="5"/>
                        <text x="200" y="155" text-anchor="middle" font-size="14" fill="#1976d2">多头自注意力</text>
                        
                        <rect x="70" y="200" width="260" height="40" fill="#90caf9" stroke="#1976d2" rx="5"/>
                        <text x="200" y="225" text-anchor="middle" font-size="14" fill="#1976d2">前馈网络</text>
                        
                        <rect x="70" y="260" width="260" height="60" fill="#bbdefb" stroke="#1976d2" rx="5"/>
                        <text x="200" y="295" text-anchor="middle" font-size="14" fill="#1976d2">多头自注意力</text>
                        
                        <rect x="70" y="340" width="260" height="40" fill="#90caf9" stroke="#1976d2" rx="5"/>
                        <text x="200" y="365" text-anchor="middle" font-size="14" fill="#1976d2">前馈网络</text>
                        
                        <text x="200" y="430" text-anchor="middle" font-size="12" fill="#666">... N层</text>

                        <!-- 解码器 -->
                        <rect x="450" y="100" width="300" height="400" fill="#fff3e0" stroke="#f57c00" stroke-width="2" rx="10"/>
                        <text x="600" y="90" text-anchor="middle" font-size="18" font-weight="bold" fill="#f57c00">解码器 (Decoder)</text>
                        
                        <!-- 解码器层 -->
                        <rect x="470" y="120" width="260" height="50" fill="#ffcc02" stroke="#f57c00" rx="5"/>
                        <text x="600" y="150" text-anchor="middle" font-size="12" fill="#f57c00">掩码多头自注意力</text>
                        
                        <rect x="470" y="190" width="260" height="50" fill="#ffb74d" stroke="#f57c00" rx="5"/>
                        <text x="600" y="220" text-anchor="middle" font-size="12" fill="#f57c00">编码器-解码器注意力</text>
                        
                        <rect x="470" y="260" width="260" height="40" fill="#ff9800" stroke="#f57c00" rx="5"/>
                        <text x="600" y="285" text-anchor="middle" font-size="14" fill="#f57c00">前馈网络</text>
                        
                        <rect x="470" y="320" width="260" height="50" fill="#ffcc02" stroke="#f57c00" rx="5"/>
                        <text x="600" y="350" text-anchor="middle" font-size="12" fill="#f57c00">掩码多头自注意力</text>
                        
                        <text x="600" y="430" text-anchor="middle" font-size="12" fill="#666">... N层</text>

                        <!-- 连接箭头 -->
                        <path d="M 350 300 L 450 220" stroke="#666" stroke-width="2" marker-end="url(#arrowhead)"/>
                        
                        <!-- 输入输出 -->
                        <rect x="70" y="520" width="260" height="30" fill="#e8f5e8" stroke="#4caf50" rx="5"/>
                        <text x="200" y="540" text-anchor="middle" font-size="14" fill="#4caf50">输入嵌入 + 位置编码</text>
                        
                        <rect x="470" y="520" width="260" height="30" fill="#fce4ec" stroke="#e91e63" rx="5"/>
                        <text x="600" y="540" text-anchor="middle" font-size="14" fill="#e91e63">输出概率分布</text>

                        <!-- 箭头标记定义 -->
                        <defs>
                            <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#666"/>
                            </marker>
                        </defs>
                    </svg>
                </div>

                <div class="grid">
                    <div class="card">
                        <h3><span class="card-icon">🏗️</span>整体架构</h3>
                        <p>Transformer采用<span class="highlight">编码器-解码器</span>架构，完全基于注意力机制，摒弃了循环和卷积结构。</p>
                        <p><strong>核心优势：</strong></p>
                        <ul style="margin-left: 20px; color: #666;">
                            <li>并行计算能力强</li>
                            <li>长距离依赖建模优秀</li>
                            <li>训练效率高</li>
                        </ul>
                    </div>

                    <div class="card">
                        <h3><span class="card-icon">🔄</span>编码器</h3>
                        <p>由N=6个相同层组成，每层包含：</p>
                        <div class="formula">
                        1. 多头自注意力机制<br>
                        2. 位置前馈网络<br>
                        3. 残差连接 + 层归一化
                        </div>
                        <p>输出维度：d_model = 512</p>
                    </div>

                    <div class="card">
                        <h3><span class="card-icon">🎯</span>解码器</h3>
                        <p>同样由N=6个层组成，每层包含：</p>
                        <div class="formula">
                        1. 掩码多头自注意力<br>
                        2. 编码器-解码器注意力<br>
                        3. 位置前馈网络<br>
                        4. 残差连接 + 层归一化
                        </div>
                        <p>支持自回归生成</p>
                    </div>

                    <div class="card">
                        <h3><span class="card-icon">📍</span>位置编码</h3>
                        <p>由于没有循环结构，需要显式添加位置信息：</p>
                        <div class="formula">
                        PE(pos,2i) = sin(pos/10000^(2i/d_model))<br>
                        PE(pos,2i+1) = cos(pos/10000^(2i/d_model))
                        </div>
                        <p>使模型能够理解序列中的位置关系</p>
                    </div>
                </div>
            </div>

            <!-- 注意力机制 -->
            <div id="attention" class="tab-content">
                <div class="attention-visual">
                    <h3>🎯 注意力机制核心公式</h3>
                    <div class="formula" style="font-size: 18px; text-align: center; margin: 20px 0;">
                        Attention(Q,K,V) = softmax(QK^T/√d_k)V
                    </div>
                </div>

                <div class="grid">
                    <div class="card">
                        <h3><span class="card-icon">🔍</span>自注意力机制</h3>
                        <p><span class="highlight">Self-Attention</span>允许序列中的每个位置关注序列中的所有位置。</p>
                        <div class="formula">
                        Q = XW^Q  (查询矩阵)<br>
                        K = XW^K  (键矩阵)<br>
                        V = XW^V  (值矩阵)
                        </div>
                        <p>通过Q、K的点积计算注意力权重，用于加权V。</p>
                    </div>

                    <div class="card">
                        <h3><span class="card-icon">🎭</span>多头注意力</h3>
                        <p>将注意力机制并行运行h=8次，每次使用不同的线性投影：</p>
                        <div class="formula">
                        MultiHead(Q,K,V) = Concat(head_1,...,head_h)W^O<br>
                        head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
                        </div>
                        <p>允许模型关注不同位置的不同表示子空间。</p>
                    </div>

                    <div class="card">
                        <h3><span class="card-icon">🎪</span>掩码注意力</h3>
                        <p>在解码器中使用，防止位置i关注到位置i之后的信息：</p>
                        <div class="formula">
                        mask_ij = -∞ if j > i<br>
                        mask_ij = 0 if j ≤ i
                        </div>
                        <p>确保生成过程的自回归特性。</p>
                    </div>

                    <div class="card">
                        <h3><span class="card-icon">🔗</span>交叉注意力</h3>
                        <p>解码器中的编码器-解码器注意力：</p>
                        <div class="formula">
                        Q = 解码器隐藏状态<br>
                        K,V = 编码器输出
                        </div>
                        <p>允许解码器关注输入序列的所有位置。</p>
                    </div>
                </div>

                <div class="detail-section">
                    <h3>注意力计算流程</h3>
                    <div class="layer-stack">
                        <div class="layer-box">输入序列<br>X</div>
                        <span class="arrow">→</span>
                        <div class="layer-box">线性变换<br>Q,K,V</div>
                        <span class="arrow">→</span>
                        <div class="layer-box">注意力计算<br>QK^T</div>
                        <span class="arrow">→</span>
                        <div class="layer-box">Softmax<br>归一化</div>
                        <span class="arrow">→</span>
                        <div class="layer-box">加权求和<br>×V</div>
                    </div>
                </div>
            </div>

            <!-- 核心组件 -->
            <div id="components" class="tab-content">
                <div class="grid">
                    <div class="card">
                        <h3><span class="card-icon">🧠</span>前馈网络</h3>
                        <p>每个编码器和解码器层都包含一个全连接前馈网络：</p>
                        <div class="formula">
                        FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
                        </div>
                        <p>内部维度：d_ff = 2048，输出维度：d_model = 512</p>
                    </div>

                    <div class="card">
                        <h3><span class="card-icon">🔄</span>残差连接</h3>
                        <p>每个子层周围都有残差连接，然后进行层归一化：</p>
                        <div class="formula">
                        LayerNorm(x + Sublayer(x))
                        </div>
                        <p>有助于训练深层网络，缓解梯度消失问题。</p>
                    </div>

                    <div class="card">
                        <h3><span class="card-icon">📏</span>层归一化</h3>
                        <p>对每个样本的特征维度进行归一化：</p>
                        <div class="formula">
                        LayerNorm(x) = γ(x-μ)/σ + β<br>
                        μ = mean(x), σ = std(x)
                        </div>
                        <p>稳定训练过程，加速收敛。</p>
                    </div>

                    <div class="card">
                        <h3><span class="card-icon">📝</span>词嵌入</h3>
                        <p>将输入和输出token转换为d_model维向量：</p>
                        <div class="formula">
                        Embedding: vocab_size → d_model<br>
                        权重共享：输入、输出、预softmax线性变换
                        </div>
                        <p>嵌入权重乘以√d_model进行缩放。</p>
                    </div>
                </div>

                <div class="detail-section">
                    <h3>组件协作流程</h3>
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>组件</th>
                                <th>功能</th>
                                <th>输入</th>
                                <th>输出</th>
                                <th>参数量</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>词嵌入</td>
                                <td>token → 向量</td>
                                <td>token序列</td>
                                <td>d_model维向量</td>
                                <td>vocab_size × d_model</td>
                            </tr>
                            <tr>
                                <td>位置编码</td>
                                <td>添加位置信息</td>
                                <td>嵌入向量</td>
                                <td>位置感知向量</td>
                                <td>0 (固定函数)</td>
                            </tr>
                            <tr>
                                <td>多头注意力</td>
                                <td>序列建模</td>
                                <td>Q,K,V矩阵</td>
                                <td>注意力输出</td>
                                <td>4 × d_model²</td>
                            </tr>
                            <tr>
                                <td>前馈网络</td>
                                <td>非线性变换</td>
                                <td>注意力输出</td>
                                <td>变换后向量</td>
                                <td>2 × d_model × d_ff</td>
                            </tr>
                            <tr>
                                <td>层归一化</td>
                                <td>特征归一化</td>
                                <td>任意向量</td>
                                <td>归一化向量</td>
                                <td>2 × d_model</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <!-- 实现细节 -->
            <div id="implementation" class="tab-content">
                <div class="detail-section">
                    <h3>PyTorch实现示例</h3>
                    <div class="code-block">
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # 线性变换并重塑为多头
        Q = self.W_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        
        # 计算注意力
        attention = self.scaled_dot_product_attention(Q, K, V, mask)
        
        # 合并多头
        attention = attention.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model)
        
        return self.W_o(attention)
    
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention_weights = F.softmax(scores, dim=-1)
        return torch.matmul(attention_weights, V)
                    </div>
                </div>

                <div class="grid">
                    <div class="card">
                        <h3><span class="card-icon">⚙️</span>训练技巧</h3>
                        <p><strong>学习率调度：</strong></p>
                        <div class="formula">
                        lrate = d_model^(-0.5) × min(step^(-0.5), step × warmup_steps^(-1.5))
                        </div>
                        <p><strong>其他技巧：</strong></p>
                        <ul style="margin-left: 20px; color: #666;">
                            <li>Dropout: 0.1</li>
                            <li>Label Smoothing: 0.1</li>
                            <li>Warmup Steps: 4000</li>
                        </ul>
                    </div>

                    <div class="card">
                        <h3><span class="card-icon">📊</span>模型规模</h3>
                        <p><strong>Transformer Base：</strong></p>
                        <ul style="margin-left: 20px; color: #666;">
                            <li>层数：N = 6</li>
                            <li>模型维度：d_model = 512</li>
                            <li>前馈维度：d_ff = 2048</li>
                            <li>注意力头数：h = 8</li>
                            <li>参数量：65M</li>
                        </ul>
                    </div>

                    <div class="card">
                        <h3><span class="card-icon">🚀</span>优化策略</h3>
                        <p><strong>计算优化：</strong></p>
                        <ul style="margin-left: 20px; color: #666;">
                            <li>并行计算注意力</li>
                            <li>缓存K,V矩阵</li>
                            <li>梯度累积</li>
                            <li>混合精度训练</li>
                        </ul>
                        <p><strong>内存优化：</strong></p>
                        <ul style="margin-left: 20px; color: #666;">
                            <li>梯度检查点</li>
                            <li>动态padding</li>
                        </ul>
                    </div>

                    <div class="card">
                        <h3><span class="card-icon">🎯</span>应用场景</h3>
                        <p><strong>NLP任务：</strong></p>
                        <ul style="margin-left: 20px; color: #666;">
                            <li>机器翻译</li>
                            <li>文本摘要</li>
                            <li>问答系统</li>
                            <li>语言建模</li>
                        </ul>
                        <p><strong>扩展应用：</strong></p>
                        <ul style="margin-left: 20px; color: #666;">
                            <li>Vision Transformer</li>
                            <li>语音识别</li>
                            <li>蛋白质结构预测</li>
                        </ul>
                    </div>
                </div>

                <div class="detail-section">
                    <h3>性能对比</h3>
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>模型</th>
                                <th>BLEU (EN-DE)</th>
                                <th>BLEU (EN-FR)</th>
                                <th>训练时间</th>
                                <th>参数量</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>RNN + Attention</td>
                                <td>24.6</td>
                                <td>39.2</td>
                                <td>12天</td>
                                <td>~100M</td>
                            </tr>
                            <tr>
                                <td>ConvS2S</td>
                                <td>25.2</td>
                                <td>40.5</td>
                                <td>9天</td>
                                <td>~100M</td>
                            </tr>
                            <tr>
                                <td><strong>Transformer Base</strong></td>
                                <td><strong>27.3</strong></td>
                                <td><strong>38.1</strong></td>
                                <td><strong>3.5天</strong></td>
                                <td><strong>65M</strong></td>
                            </tr>
                            <tr>
                                <td><strong>Transformer Big</strong></td>
                                <td><strong>28.4</strong></td>
                                <td><strong>41.8</strong></td>
                                <td><strong>3.5天</strong></td>
                                <td><strong>213M</strong></td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </div>
    </div>

    <script>
        function showTab(tabName) {
            // 隐藏所有标签页内容
            const contents = document.querySelectorAll('.tab-content');
            contents.forEach(content => content.classList.remove('active'));
            
            // 移除所有标签页的active类
            const tabs = document.querySelectorAll('.tab');
            tabs.forEach(tab => tab.classList.remove('active'));
            
            // 显示选中的标签页内容
            document.getElementById(tabName).classList.add('active');
            
            // 添加active类到点击的标签页
            event.target.classList.add('active');
        }
    </script>
</body>
</html>